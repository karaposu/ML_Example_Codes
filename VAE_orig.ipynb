{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_orig.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UIgszK9MjnGM",
        "outputId": "a6982a93-bb1c-4ced-f297-4bc555ae5d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 550.007996\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 182.565079\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 158.018173\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 152.305542\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 128.662598\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 129.949722\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 131.803040\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 128.187347\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 122.973801\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 121.044891\n",
            "====> Epoch: 1 Average loss: 146.7496\n",
            "====> Test set loss: 119.1544\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 122.132347\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 122.975044\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 112.275848\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 117.202362\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 121.091209\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 122.420563\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 108.191025\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 118.630280\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 111.876175\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 113.125916\n",
            "====> Epoch: 2 Average loss: 115.6025\n",
            "====> Test set loss: 112.0064\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 112.445122\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 119.105606\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 108.188400\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 111.990227\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 114.713898\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 114.493027\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 109.375870\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 109.224922\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 116.059242\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 110.501328\n",
            "====> Epoch: 3 Average loss: 111.2733\n",
            "====> Test set loss: 109.5829\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 108.771103\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 114.140038\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 111.953499\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 112.644325\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 113.023941\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 113.080612\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 114.364311\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 109.281517\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 106.599945\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 109.674591\n",
            "====> Epoch: 4 Average loss: 109.3108\n",
            "====> Test set loss: 108.0895\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 111.111343\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 112.075371\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 106.320633\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 105.533661\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 106.510078\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 103.404892\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 112.033562\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 109.529678\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 111.865204\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 110.879318\n",
            "====> Epoch: 5 Average loss: 108.1918\n",
            "====> Test set loss: 107.2432\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 106.280151\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 108.950523\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 111.080109\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 107.237793\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 106.301910\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 103.519913\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 114.059593\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 109.276962\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 105.932419\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 109.790482\n",
            "====> Epoch: 6 Average loss: 107.3605\n",
            "====> Test set loss: 106.6135\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 113.145187\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 105.055252\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 108.563324\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 101.490181\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 107.633743\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 105.380173\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 105.430252\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 103.928230\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 108.792267\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 105.076828\n",
            "====> Epoch: 7 Average loss: 106.7602\n",
            "====> Test set loss: 106.1093\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 101.151253\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 104.880516\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 110.021851\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 105.617538\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 106.256790\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 108.214264\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 97.493675\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 104.367760\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 103.675476\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 108.521034\n",
            "====> Epoch: 8 Average loss: 106.2360\n",
            "====> Test set loss: 105.5736\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 107.223450\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 105.319687\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 106.646652\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 109.148041\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 107.101524\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 108.901024\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 110.817795\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 99.801895\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 108.923828\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 104.725006\n",
            "====> Epoch: 9 Average loss: 105.8597\n",
            "====> Test set loss: 105.5036\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 108.666245\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 110.634071\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 104.114059\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 106.575638\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 102.847931\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 107.157448\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 101.703522\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 111.096123\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 108.913269\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 104.192307\n",
            "====> Epoch: 10 Average loss: 105.5100\n",
            "====> Test set loss: 105.3913\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 105.863731\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 103.756538\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 100.612152\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 104.089798\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 102.446167\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 105.677368\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 110.345993\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 112.040390\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 102.639114\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 97.258095\n",
            "====> Epoch: 11 Average loss: 105.2298\n",
            "====> Test set loss: 104.7959\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 106.796951\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 103.688805\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 106.674141\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 101.892250\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 104.284958\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 105.387230\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 107.144371\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 104.205048\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 99.897171\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 110.438217\n",
            "====> Epoch: 12 Average loss: 104.9811\n",
            "====> Test set loss: 104.7848\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 112.893890\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 110.324402\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 106.427971\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 98.900154\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 102.370819\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 97.833580\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 107.456436\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 103.945663\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 106.673225\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 105.644775\n",
            "====> Epoch: 13 Average loss: 104.7436\n",
            "====> Test set loss: 104.5513\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 103.378822\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 106.166046\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 100.738129\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 103.906296\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 104.926872\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 102.658340\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 110.634979\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 101.212204\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 105.586998\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 104.734001\n",
            "====> Epoch: 14 Average loss: 104.5278\n",
            "====> Test set loss: 104.1308\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 103.220123\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 108.969315\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 105.203423\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 106.121849\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 101.030701\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 106.766541\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 101.726799\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 103.022133\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 107.173988\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 103.079819\n",
            "====> Epoch: 15 Average loss: 104.3598\n",
            "====> Test set loss: 104.0990\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 104.823395\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 105.615448\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 106.792633\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 108.919113\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 107.944839\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 104.100021\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 103.143188\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 108.752014\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 105.429298\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 101.604279\n",
            "====> Epoch: 16 Average loss: 104.1469\n",
            "====> Test set loss: 103.8791\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 103.145355\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 98.389206\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 98.424225\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 105.413742\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 100.598785\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 107.085464\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 105.193069\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 99.316803\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 100.101631\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 105.907166\n",
            "====> Epoch: 17 Average loss: 103.9893\n",
            "====> Test set loss: 103.7833\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 106.624268\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 101.621483\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 107.578842\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 106.760231\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 105.529396\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 101.859810\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 111.925629\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 100.544785\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 101.630470\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 102.133011\n",
            "====> Epoch: 18 Average loss: 103.8371\n",
            "====> Test set loss: 103.9053\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 99.471191\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 104.244453\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 107.077072\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 102.072769\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 100.036911\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 102.264488\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 98.391739\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 106.564560\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 106.458893\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 106.291443\n",
            "====> Epoch: 19 Average loss: 103.7165\n",
            "====> Test set loss: 103.6404\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 99.714088\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 105.367752\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 105.458313\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 102.833046\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 104.003532\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 105.092155\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 107.618881\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 106.614548\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 102.329453\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 96.931549\n",
            "====> Epoch: 20 Average loss: 103.5964\n",
            "====> Test set loss: 103.3335\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 100.676361\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 102.720795\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 101.362740\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 103.615074\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 101.163399\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 100.199677\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 100.251648\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 102.629074\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-12222c0587fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-12222c0587fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "# parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "#                     help='input batch size for training (default: 128)')\n",
        "# parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "#                     help='number of epochs to train (default: 10)')\n",
        "# parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "#                     help='disables CUDA training')\n",
        "# parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "#                     help='random seed (default: 1)')\n",
        "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "#                     help='how many batches to wait before logging training status')\n",
        "# args = parser.parse_args()\n",
        "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(7)\n",
        "batch_size=64\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True,)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "    # see Appendix B from VAE paper:\n",
        "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "    # https://arxiv.org/abs/1312.6114\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(), './reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1,300):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, 20)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       './sample_' + str(epoch) + '.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# ! wget https://raw.githubusercontent.com/karaposu/Brain_typing/master/a.png\n",
        "\n",
        "\n",
        "image = cv2.imread('reconstruction_1.png')\n",
        "image2 = cv2.imread('reconstruction_19.png')\n",
        "\n",
        "cv2_imshow( image)\n",
        "cv2_imshow( image2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "-B2ssHQ3op3k",
        "outputId": "7b691757-2d63-4664-dbcb-8dcf47f7626d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPIAAAA+CAIAAACeINE8AAAjrklEQVR4nO1deVRTZ9q/92bfQ0jCDgGEIIqgWLW4QLW2Wq22tXQRFfDUttPRVjtHbfvVbdoZl9qpjqO12DN2rEdtnap1LYJblVYtKEoUMayyJEDIHrLee78/njP3pIAYknSWc/L7wxNvLs/73Pc+7/P+nuXeIEgIIYQQQgghhBBCCCGE8Nth2rRpBEEUFBT8pxUJIQR/gA14dN68eSRJZmRk/Ju1CSGEoGBgsxaJRAiCXLhw4bcefvLkyUajkSAIkiQJgqitrS0uLo6Pjw/uKF9//XVRUVFwZfbH6dOncRwnSfLkyZO/9Vj/tcjNzSVJEsdxHMe//fbb3NzcoIiNi4tTq9VarTY9Pd1/KaDZu+++GxSdBkFqauoHH3zwwQcfnDp1qra2tqOjA8fxnp6eY8eORUdHB2sUgiCcTmdiYmKwBPaBQqGoqKgwm80ejwfHcbPZ/MYbbwQikE6njxgx4o9//OOBAwdwHL9w4YJCoQiSsr/CmjVr6uvrSZJsbW2NjY0NUJpCoejp6cFx3PMv6PX6GTNmBK7npEmTCIIgCCI/P99/KdeuXcNx/MCBA4ErNCRERERMmDDhq6++crvdXV1dL730EoYNvJ8MCQRB4Dg+bNiwwEX1AZPJfO+991QqFXUj4abW1tb6LTMvL6+srMxbmsfj6enpCbplv/nmmw0NDfi/8Lvf/S4QaQqFYteuXd46w+fz58/zeLwAVQ2OWe/atQvH8bq6ugC18RuzZ8/WaDQ4jqekpAQurba29rcwaxRF33vvvf43MhCzXrJkid1uBz/32WefrVu3bu3atS0tLR6PJ6D991+Iiop69913J06c+MYbb9jtdpPJtGLFikOHDuE4/s033/gnE0XR3Nzcnp6ePkuR+lxSUhKg2sE067Vr1waoTSCYNGmS0+lcs2ZNgHKmTp3a1tYWdLPOzMzUarUPu5H+mfXixYvhz0tLS70pgVgs3rdvn1AoDFDnWbNmfffdd5R7VqlUCxcuRBBk586dOI5v3rzZP7Hbtm3znoFt27YhCCKTyaqqqgLfuwDff/89QRA9PT1jx471X4parf73cOsBQafTo6Ojf/zxRxzHP/zwwwClLViwgCRJvV4fRLK+Zs2azs7OPqZ89epV2NP9u5ExMTGw/DZv3kyn04OlKoXly5d7Uw4cx0+ePEmj0ZAAzFomky1atIji0yaTqbi4WCqVwrdSqRSOP3jwICEhwW/NIyMj79y5QxBEdXW130IQBEEOHjyI4/jVq1cfdgKNRsvPz3/7X3jhhRdggoKCuXPnwrzv2rUrcFpWUFBAEMRPP/0UFN14PN7WrVsp8wXU1dXV1tZOnjyZ8lt+mHVeXh6O4x0dHTExMQ87Jyoqas6cOc8888yQJDMYjE2bNul0OpA/duzY7u5uHMcnTpwIJ1y+fBnHcT/KFIsWLfJe2JcvX+5zAhzv7e1dsGDBUIVTmDp1KjCQxYsX+y0EQRDk5ZdfxnHcYDAMHz68/7fz5s1TqVT4r3Hr1q3HH388kEE5HE5hYeHx48edTmd9ff2MGTNQFA1EIGDTpk1BNOuZM2d630iNRrN06VLq2ylTpgRi1h6Pp6ampv9XbDY7Pz//u+++a21t9Xg8QyWHLBarrq4Ox3GLxVJcXCyTyfbs2dPW1kbxnG+//RbH8ZUrVw5V57q6Omo2bt++3d8lB4WEHDlyBMx6zpw5gchB4uLitFotjuPe9wyQnp5usVjwgdDQ0ODHwHQ6PS8vb8eOHQ0NDR6P58qVK++++65MJgvoArxw6tSpoJg1iqLLli3r7OyEiyVJsrGxsQ/1p7K2fgRJCoVCq9W6XK533nmHOpiQkLBu3boHDx6A9TQ1Na1du3aoyeDCwkLQmdKKyWROnDiRz+cjCBIbG9ve3t7W1iaRSHyXiaLo4cOHqVtfXV1NcQ8K27dvh9nQarWZmZlD0tkbFRUVwTFrkIXj+Lfffut9kEaj3b59G67k7Nmzy5YtW7Zs2cqVK5uamuDgzp07fREukUgKCwvXrl1bXl4OOyBBEHv27AlW9t4bwTLr9PR0jxdwHB8/fnyfcygS8vTTT/sxRGlpKY7jzc3NK1euzMvL27179/379z0ej91uP3fu3Llz55KTk4cqk8lk1tfX4zh+8ODB/owOw7D9+/e7XK5Dhw4NSaz3bNy+fbu/TfN4vKNHj1JUrb9/9BHh4eEqlYogiLt37w5p4Q2MAc06Pz+fsmmoRALCwsLOnz8Pxx8pefbs2UQ/kCR54cIFpVIZqN6/hkKhuH79OkmSW7duDUROZmYmbCYUenp6+lRD4ZxAzFoikdy6davP4tm8eXNqaqp/anO53NWrV+M4Xl9fP6C/VCqVarVao9G8+uqrPsqkogvQsKqqasBwMCcnh6JqDzvHF8ycORMs5PDhw/5J+BXGjBmj0+mcTqd3FeDtt98Gs+6/7oFt++IUU1JSoLI4f/782NhYKDQ+ePAAx3G9Xg8pp2Bh2bJlsBVERkYGIufUqVOeX2PKlCkDnhOIWSMIAmVFCoHojCDIhg0bcBy32WwPI3XgUKdOneq7TCq6AD/9MHsNymwgCHLw4EGCIFQqVf8NwU9s3boVx/E9e/ZQRyizZjAY1EGlUvm3v/0NCLePJKQ/RCJRSUkJsLTAsx8UwKzr6uo4HI7fQjZu3Ohtaj09Pf1tWqFQwMIOpCdk7969uFeCxePx+K0zgiATJ06srKzEcfyjjz4a8ITs7GyIzqdNm+ajzMzMTCq6wHF89erVDzszKB0yIpFIpVKRJNk/x+I/5s+fj+N4R0cHxTeWLFkC1yMQCBAESUxM/PLLL00mExw0Go2FhYV+DycQCMrKynAc/+STT4JzAQiyY8cOHMePHDkSiBAq3ofUR3/fo1Aobt68SdGGLVu2DHUIDMNWr17tdDqbmpoKCgrmzZtns9k8Hk8gKa2vvvoKx/GrV68+bElv27bNbDbPmzfPd5nLli2jpmLVqlXe3s0bubm5VIfMiRMn/NEeQRAEmTt3LjAQv6n5AODz+dXV1WDZzz///JNPPvnUU0/duXMHx/EbN24cPnzY6XRSC7e3tzfwzrs333wThAdF/5iYGL1ej/uVkfWGt1nPnj27zxApKSnePSH9ObcvkMvl8OfPPfccHJk3b57ZbC4rKxOLxX7onJiYCC0DD6s2b9q0qbm5ecKECUOS732lDztHLBZTnA3H8T4zNiQUFRWBWT/xxBN+CxkA0DiKDwqj0fjJJ5+MHj3aF4EsFmuQb7dv3x5Es16/fj1MSoC9e95m7Z2LWLNmDURO3syhPz/xBVOnTvV4PH2qaDU1NTiO+5EdYjAYCxcu9Hg8JSUlfcpkTCYzNjb2hx9+0Gq1xcXFQ60MeF/pgCe8+OKLFy5coM5ZunSpd2phqLh9+zbkQKKiovwWMjAmTpx45coVu93e36Cbmpr279+flZXlo6hFixb96U9/GuRbqIQFi4SsX78ex/HOzs4A5Xz55ZfUJdfU1FAcmjrod666j6qTJk2ijvB4PLVaTZKkH2YdFxcHM7lhwwbv40qlEhJW+/fv968N9eLFiwO6tv6zgeN4gP362dnZOp2OIIjly5cHImcwrF27dtSoUQUFBTt37ty5c2dBQcGoUaPkcvmQhHz00Uc1NTWvvPJKn54HmUx29OhRoDRlZWXBChnBVvbt2xegnLFjx2o0mj6ZEG+/tXfv3uTkZC6X6/cQCxYs8Hg8VJc2m80+fPiwx+MxGAx+NO6FhYXdvXsXzJc6uG/fPiixffzxx35Xu7y59cNmAz6fOHEiED+NIMjevXths83JyQlEzm8OqVQKXTVqtfrevXunTp26d+/evXv3Ojo6CIJobW3dsWMHBKNBwauvvor/OpPjN/bs2aPT6frcSJPJVFtb+/HHHw/YYjAkSKVSm81mtVrXr1+/bt06ipu+9dZbfkijzLq8vHzSpEknT550Op3A++fNmxdIS0JCQsKKFSv6pPD7mHVJScnTTz8deD7u3r178MDUIE0y/y2g0WgLFy7cu3cvJGi1Wq3BYDhz5sySJUsGp91+gMfjnT59OihPZyAIMmXKFO8beeHChWeffTYokgEQI3rbRyCb7+zZs/sERT///HMgFWxviMXipUuXLl26tE9rbp8OmQABZv2HP/whWAJD+M9g7ty5UGXcs2fPs88+G8g65/P5H330EXQ6fP755ykpKYFk7kMIIYQQQgghhBBCCCGEEEIIIYQQQvivxSMy85C6pxL4JEn2+RBCCP+FGMysaTSaRCIZP358UlLStWvXNBoNn883mUx6vd7pdIJl/6/Yt3dp7d+gM4qi/yszgwzktv7X8dD3UdDp9IiIiCeffHL69OlOp9NqtaIo2tHRYbFYgjgL1G5AgUajeTweeAwsiKNgGAZj4QE/cvLIgTAMg1H+J6wEph3DMIIgYDUGS22Y8P/IJAxs1nB5KIqGh4eTJOl0OrVarVarNZvN8CqtYJkak8mMjo6Ojo5OS0tTKBRcLlcoFFqt1qqqKlhCGo3GYrGYzWZvQ/dRPvwrEolmzZrFYrFUKlVDQ4PBYAiuZcNSDA8Pz8zMhGbG+vr627dvNzc3Q6eOfzLpdDqTyeRyuRiGmc1ml8s11BkYRDjoLBKJEhMTU1NTWSyWxWKB+cFx3L+B4IYyGAy5XD5z5sysrCwGg1FbW3v16tXa2lqj0ej7goF7R71+EVSNi4uTSCTQpwrvORlEwkO9NYqiHA6nt7e3paWlrq6upaXFYDD09vYGbhOgNJ1Ol0gk06dPT09PT09Pp9Pp8fHxNBqNyWSSJDl37lyn03njxo3S0tJz585ZLBb/BmIwGNHR0aNGjUpMTMQwrLGxEdxSsPYZGo0mFovnzJnz4osvZmZmcrlck8lUV1dXVlb297//3Wg0DomNgMEJBIKcnJznnnsuKSmJy+Wq1eo7d+58//33HR0dTqeTeqLZP54DxsdisVJSUh5//PH58+fL5XKbzdbW1nbmzJmenp6enh4/ZMKNi4iIeOGFFwoKCmJiYng8Ho1Gs1qtarX64MGDe/futdlsvktjsViwb2MYJhKJUlNT58+fn5GRodFoSkpKysvLh2zWMLkcDkcul/N4vJaWlpaWFpPJ5D2bAe4vMLlcLre3t7ezs9NoNHZ1dblcLg6HIxAI0tPT8/LyWCyWQqGIi4tzOBx+GCKcj2GYTCaD3s6enh544NI/nfvoj6Iok8lUKpVvvPHGs88+K5FImEwmgiBsNpvL5ZrN5mPHjplMJsTnWYLbKZVK33777cLCwrCwMARB3G63RCIRiUTt7e1VVVVarba3t5cS6IdloyjKZrMTEhJyc3MnT54cExMjFAqlUimHw7l27RqGYX7cUwzDOBxOTk7O+++/n5mZyWazaTQaPL4gFotHjhyZl5d36NAh380awzAajQYbncfjAd6rUCh87xEf2FuDK01KShKJRB6Ph8lkwo5Ap9O9WWN/4/ZlUmBVYBhmt9tv3bpVWVlpsVjsdrvb7YYhFApFR0dHYWGhRCIxmUwul8u/9YOiKJfLHTVqFJvNbmpqgr6fYHFHOp0+cuTILVu2jB8/HgwaZoZGo/H5fIVCERMT09raChfli6oYhoWFhS1ZsqS4uBg6lS0Wy40bNy5fvnzp0iUURSUSSW9vr8fjAZ/tx4XA9hUZGVlYWDhq1CgajVZXV5eQkCCXyzkcDhj3UJcKhmF8Pn/GjBnvv//+sGHDGAwGiqIej8dms7FYLDqdDiM+7KnH/hoCICcBz+DgOO5yuZKSksRisdPptNlsj9RwYG8tEAhSUlIyMjISExN1Op3FYoHncIGEuN1ugiCAGTMYDDA7mGsfQyV4gMJqtYJYiBHhK2i8ttvtDofD5XJFREQEQuWFQiGfz3e5XDU1NfDItH9yvAFudfTo0Rs3bszJyWEwGHADzGYzgiB8Ph/DMIVCkZ6efuPGDbfb7YuhoCjK5/Nzc3Pz8/NZLJZWqy0rK9u9e7darXa5XDQaTSgUZmdnR0ZGVlVVwc7mh+ZMJhPeBj9ixAiTyVRTU1NfX2+z2Xg8HpvNlsvlsOwRn/cBDMOEQmFhYeGqVavEYjGDwcBxXKPR3Lhx48GDBzk5OSNGjGAymXFxcTExMRqNxkc9qV+nAB0IgmCxWBKJBKa6qanpkRHLr8ya4osikeixxx4bMWJEcnJybW2tTCbj8XhhYWFAoeCctLQ0oVDI4/GMRqPD4bh9+7ZOp7PZbD4GSfALAaC995+gKBoWFpaWlubxeLq7u8+cOeNwOHycjj5AURTH8ba2tjt37pw7dw627wBdNXCPxx9//M9//nNWVhZEAh6PR6fTlZeXc7ncxx57TCKR4Dg+fPhw3xMLYLiTJ0+2Wq0XL17861//WllZSe3aHo8HRdGUlJTIyEij0djT0+MH/eDxeEVFRcXFxYmJiW63u7GxsaKioq6uTqvVRkVFicVirVYLTy35KBnDsIiIiK1bt86aNQueDHK73Q0NDTt37jx37pzD4VCpVKtXr46Pj+dwOOPGjauqqvLF31EGDYB9bOrUqTDE1atXu7q6hmbWFMLDw4cNG5aSkgJGbLFYwENzOJyIiAgIn8eMGQO83mazORyO0tLSS5cuNTU1+eJIQGNgct6XCixt2rRpqampBoNhx44d1dXVficTEAQxGAxms1mv1/u9NrwBwdaLL764YcOGyMhIFotFkqTL5WpoaNi2bZtarY6KirJarU899RSCIFKpdEhvkWUymTabrby8/NixY/fv36c4NGRFsrOzc3JyJBJJTU0Ng8Gw2+2+WzaKokKhcPv27U8++aRAILDb7Wq1evv27ZWVlQRBiMVilUqVlZXV3NxstVp9n4rk5ORdu3ZNmjQJnGhvb++VK1d27NhRUVFht9tpNNrNmzd1Ol1CQgKHwxk7diydTvfdNhCv1cXn82fNmoWiqNFo3Lhxoy9CfmXWsDgYDEZycrJMJkNRtLOzs7y8/IcffjCbzW63WyqVJiUlpaenZ2VlhYWFsdlsh8MhEomcTuerr74qkUhKSkr0ev0jR6UyOJQ/Q1GUxWKJxeLJkyc/8cQTOp3u9OnTFy9e9I6QhgqwOafTqVQq4+PjIZOA+BvpAolcvnz50qVLhUIhxOlut1ulUu3bt6+0tLS3t7etrU2pVDqdTrfbffbsWavV6stY1CbZ2Nio0Wi6u7uB11HHU1NTCwoKJBJJbW1tfX29j2Ip4WKx+MCBA+PGjWOz2SaT6aeffvr8889VKpXJZOLxePB2aovF0tPT46NkDMMSEhL+8pe/TJw4kclk4jje3d29e/fuQ4cOtbW1wSZMkqTBYIDwkSAIq9XqIwP0DtsA2dnZsbGxBEFcv369oaHBFw0H8NYYhsEbzk0mk0qlqqysNJlMEAEIhcKoqKhhw4bBb7uw2WwMwwQCAeQBxo0bd/78eXiT0OCjwlbO5XKZTCZBEAwGIy4uTiqVSiSStLQ0DMNu3rxZWlrqS3Aw+CgsFkupVGZlZXV2dt65cweeUPJPVGRkZH5+/jvvvCMSiQiCgKjozp07u3btunLlitFopNPpfD5/5MiRTCbT5XI1NTX5qDwYgdlsbmxsNBqNHA6Hz+cDnYOsxe9///uIiIjm5uaTJ0/ev3/f97wQiqI8Hg98Ko1GMxgMR48e/eyzz+CJYwRBBAKBWCyOiIhwu93wwprBJUM8J5PJtm3bNmXKFOBgDx482LBhw+nTp+GhNZAAWR14DJFGoxmNRl8U9p4TsGwul7tgwQLINZ84ccLH29fXrKnUG0EQXV1der3ebrc7nU46nS4Wi0eMGJGYmMhkMhsaGsxmc1tbm1wuHzlyZFpamkAgGD58eFpa2u3bt+12++BTA1Ry1KhRSqUyIyNDLpczmcz29na32+1wOO7evXv37l2HwwGTiATgXyFBGxcXN3r0aKlUajKZ/EuKCYXClStXvvTSS3w+H8i0VqutqKg4cuRIW1ub1WoF+hQXF5eens7lcltaWhobG32PUGFjsVqtMpksJiYmISHBZDKBwOjoaKFQWFFRUVVVdevWrd7eXh9ZGfjpxYsXT5gwwePxtLa2bt68+fvvvwemQZIkk8kMDw+fMGFCXFycyWSC31fwZSqKiopyc3O5XK7H42lqalq+fPmPP/4ITpqqgrFYrLy8PJFIhGGYx+O5fPmyH2SSTqcPHz4ckjYWi6WqqgpMYsiZEJIkgboJBAKCIBwOR1hYGDxUx+FwaDRae3u7wWCAFhG3283hcFwuV3R0dFhYGIPBSE1NFQqFg5g1LBsej5ednf3888+np6dHRUVB0kqj0TCZzK6uLjqdTrFh4N9+F1BYLFZGRgaLxYLEkLc38nHBACubMWPG/PnzJRIJZK+6urqOHTt25syZe/fuud1ut9vN4/GUSuXChQujo6NJkmxvb4e6mo96kiQJud6srCx43SaCIM3NzU1NTUaj8Ztvvrl3755Op7NarT66KzCsgoKC1157TSKRdHd3l5SUnDx50mq1QpEcDBQqozwer7Gx0WQyPVJhBoMxcuTIoqIieOmFwWDYuHHjlStXqNCFohASieSZZ56BvJ7H4+no6PDDm0BQkZSURKPR9Ho9UBpfsm0DkBCXy8VisXAcZ7PZUqkUShiQr2hoaID+EHhlssfjkUgkFoult7fX6XQC5ZJIJIO8cQZctUQikUqlJEkajUYI6err67u6uiAI4/F4er3e7XZT4fCQpoMCOA86ne50OisrKzs7O/1YHpCtKyoqEolEKIoCTbx48eKWLVtMJhO0f6AompGR8f77748dO5ZGo+l0ui+++GLwLasPwAPJZLLk5GRI5Nlsto6OjubmZo1Go1arjUYj1M998Xlw1UlJSYsXL5bJZA6H45dffjl79qzFYqEcKp1OHzdu3KxZs6Kiotrb28+ePdvS0vLIyYmKinr99dcjIyMhtXzz5s2zZ8/a7XYQS3kKDoczc+ZM+PkigiCamppaW1t9nw0KXC534sSJDAYDauatra00Go1quRkEA2dCdDodkDySJKOjo+G35AQCAY1GM5lMQCHgK7FYPH78+KioKAjO4Ed9BpQJ18xgMAQCAZfLbWtrO378uF6vNxgM3d3d8K7e6dOnJycnm0wmYD4BJuNIkoQYQCKRVFdXe5d1UN/elQHp5Pz8/DFjxsBsGgyGffv2ffrppz09PWBhUD9fsWLFpEmT6HS61WotLS29fv36kHLkMJBEIoEpJUlSp9NpNBqdTmcwGKg6q48BKIZhiYmJ8+fPh6pCXV3d3r17W1pagPjC/jNixIiFCxdmZGTo9fr9+/cfP378kZUjDMOUSmVeXh6PxwNj/eyzz3Q6nbdW4LbGjBnz9ttvw7PuNpvtgw8+8KP9AVSNiIiAbNvZs2fhVS2+bOB9zRo4Ynt7u0ajiY2NjYqKys7ODgsLA3rtdruTk5NFIhH0WnA4HLFYzOFwoFemubm5vr7+YfyVKjjD8qivrzcYDHa7narJMxgMPp8fGRlZXV3tXxNI/3lxOp2tra3Dhg3rv8P6QtEgPT958mS4kXa7/dKlSwcOHPDul2IymQsWLID4yePxtLS0bNq0aUjBLpgCh8NRKBQkSarV6qampq6uLp1OFx4ertfr4UUcvoeJHA5HqVTm5ubS6fTKysqvvvrq+vXrUC+DsTIyMoqKitLT091u99dff33ixAmz2fxI+RiGzZ07F3Ytu91+5MiRn376qU+vJZPJlMvlb731Frxf0+PxfPPNN+Xl5X4QaxRFlUplWlqa2+2+evXqyZMnXS4XLNr+ScA+GIBb2+32pqYmeM90YmLiyy+/zOFwnE5nR0eHzWaLjY0NDw+H2UEQhEaj2e12oJLV1dWlpaUDJomojB6bzRaLxQ6HA8dx+JdiYzKZLD4+XiQSwUCB0A8KGIZB203/ZCfpW2cLhPMEQbjdbpvN1traCusWwOPxZs2a9X//938QShqNxjVr1rS2tg5Vc6ii2Wy2yspKvV7f1dU1evRopVIpk8laWlqoNKiPYqESbjQaW1parl27du3aNSrQ5HA4o0ePfv3113Nzc/V6/ZYtW86cOeMLq0YQBEXR2NhYyGyq1WqqpOB9c5OSkt57773Zs2dDxVGlUq1fvx7yqkMCLM7i4uKwsDC3233hwgXKJKjJJx/e7zWAWTscjvb29jt37mRmZmIYJhaLw8LCXC6XSCQym80SiYTNZsNtxnG8p6fHaDT++OOPNTU1ly5dgorjwwaj0Wg8Hk8gEHhXy2FSmExmZGQkm81uaWmBNlTq2yGZSB92wWQymUym1Wod0P37YiiQtIKTLRYLZISMRiOTyUxMTFy1ahXE+wiC2O32HTt2nDlzZqieCXYqFEVdLpdKpdJoNBEREeHh4SwWy2q1ulyuoXalMhgMpVIpEAhaWlpqa2vBplEU5XK5M2bMWLVqlVwudzqdJSUlkJXzfR/o6upyu900Gs3hcOj1eg6HA3EXtObOnj27qKgoMzMTCjQajea1117r7Oz0wz1hGBYfHz958mQGg9HV1VVTU0OSJNWq6m3ZyEBGMoBZQ89UbW3twYMHoQkkLi4OaAZwNRRFcRzv6up68ODBhQsXrl+/XldX53A43G73IDcA2rIEAkFaWhqKor/88gtMKEmSbDY7MzPzzTffTEhIUKvV0B8DodhQbZraoeBzbGwsj8ez2+3QrTXUnhCCIPR6fWdnZ3JyMnRIZmdnh4eH22y2MWPGQHwMc93b23v06NFPP/3Uv7y4w+Ho7OyEgjl0bqSkpJAkqVKpgPAMaR6ADEgkkhEjRvzyyy9qtRqyT6+99lpBQUF4eDgk+3zkHhRwHIe376EoGhcXl5eX19vbazKZJBJJbm7uzJkzR48ezWazIbDu7u7+8MMP79696wf9AMdfXFwsl8uhRECj0aD2B+uzT6TR/xIGCBnhDywWS2VlpUql4nK5kHuSSqVisXjUqFHh4eFms7mioqKjowPCO1B9cLoDjVA0Gg3C28jIyLi4uFu3bvF4vKysrGeeeUYul7vd7u7ubmiCHUTUIPDenng83tixY9lsdmdnp8vl6t9p/Uj5JEk+ePCgrKwsOTkZkpiZmZkjR46EWYYqKWxZmzdv3r17tx+7LfIvVwLcRiaTzZw5c86cOVKptLa2trq6urOzE5JCPl4+BMrQhBQZGblixYqpU6d2dnYqlcrx48ez2eyurq6PP/74+PHjDodjSDNMEERpaWl+fv6wYcPkcvnSpUufe+45l8slFApjYmKYTCaNRoPR7969u27duvPnz/vYwNjnEphMZmpqKtTbYVyZTAaklyRJ7wSfr9zaGziO22w2m80GO3tHRweGYRUVFWi/p6conjr40nS73e3t7ZDrmTZt2vDhw+FNc2FhYQKBwGazqdXqsrKy5uZmKrvnfbVDCsIwDJPL5Ww2u7W1FQrFfc7xRRpEGj///PNjjz0GwTE8tAIOA4rG4KTb29sDaQ8EnxQVFfXKK6/k5OTExMSYTKby8vIbN24MyfjgRthstoaGBpfLBb090dHRsKQRBLl///6KFSuuXr3qn7b37t375z//uWjRovj4eC6XO2zYMJhq+NZut2u12i+//PIf//gHvGB7qI4JvBKbzc7KyuLz+eA1dDpda2ury+UCwuOLHJ9+W5tSrs9cUCUlykE+zPiotWUwGMrKyiA7gaJoVFRUTExMe3v7zZs3jUZjR0dHa2srpFf9cNXUUgZNent7VSoVPAPR5wGCIe28NTU1X3zxBUEQw4cPZ7FYbDbbarVev3799OnT165dg+JcINEtWIZQKMzIyJgyZUpMTAy0d/78889DIgkA8Jfl5eUSieTll1+GvBOCIBaLZd++fbt379br9f5pC4scfi0oPz8/Li4OQgK3293S0gLBFdQH/E7OUlFgZ2dnY2Mjl8t1Op1lZWWw31L0EnnUHQzo15epDDzyqE2BOp/6TO0pfVhB4NkPb92APsGd9q8TEPlXSEen0+VyOYqiZrPZZrPBfuK3zP56ymSy6dOnv/LKK4mJifX19V988cW5c+eg0uGHTDqdzuVyIZ1itVrv379///59KDEGOMOgLZvNlkgkUOoym81gx8F6SgP6n6FryOFwQNjW5/mV39asvf8bLIsMFvosuf9ygK1IpdL4+Piurq7GxkY/iKk3qL0UCs5BdBn/Bniblj8L+z84dgj9ATw1uLsWEro7vsObgYQQQgghhBBCCCEMEf8PNhKrY9EJDD8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=242x62 at 0x7F20C2838250>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=242x62 at 0x7F20C282D3D0>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}